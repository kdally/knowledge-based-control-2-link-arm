Questions

2.1/
a/ The number of simulation steps is determined by the simulation time divided by the time step, I.e. 10/0.05=200 steps are executed, corresponding to each discrete time instant that corresponds to a new position of the pendulum.

b/ Parameters are out of bound (epsilon, gamma, alpha, pos_states, vel_states, actions) as they are all 0.

c/ A learning rate of zero effectively prevents the model from learning anything. It is also not allowed to perform random actions, as in to do exploration which is required for any RL algorithm CHECK THIS. Also, the number of different actions the arm can take is zero, so the controller is meaningless.


2.2/
a/ If the learning rate is too high, this will result in over-fitting and the model will not handle well different type of data from the training data. In other words, it has learned too much from the training data.

2.3/
b/ Since the process is stationary (the task does not change over time) and does not rely on the sample-average method CHECK THIS, optimistic initial action-values are used. This encourages exploration in the initial trials as the reward will be less than the initial value. Here, we set all initial action-values equal to three.

2.4/
c/ Training data will not be allowed to reach more extreme values, which will make the model unable to handle more extreme validation data VERY MUCH CHECK THIS.

2.5/
a/ The simplest reward is, independently of the action, to reward the maximum reward if the pendulum is at the top with zero velocity.

